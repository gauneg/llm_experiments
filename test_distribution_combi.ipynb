{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader_asp import AspectDataset, get_dataset, MamsAspectDataset, DataLoaderGen\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from prompts import prompt_dict\n",
    "import numpy as np\n",
    "# from evaluations_extracted_terms import infer_with_dataset\n",
    "import pandas as pd\n",
    "import ast\n",
    "from selective_generation.semantic_enc import PhraseEncoder\n",
    "import torch\n",
    "# from matplotlib import pyplot as plt\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives Data Analysis Of labelled:\n",
    "Are the text examples having no Aspect Terms(AT) are semantically different from the instances having atleast 1 AT.\n",
    "\n",
    "### Experiment\n",
    "\n",
    "$t_{x_{1}}, t_{x_{2}}... t_{x_{n}}$: Text inputs\n",
    "1. Select all the text instances having ATs. \n",
    "  \n",
    "  $$ds = ds_{AT>0} + ds_{AT=0}$$\n",
    "\n",
    "2. Calculate vector representations (sentence embeddings) for all the text samples.\n",
    "\n",
    "<!-- $f_{embed}(t_{x_i}) = V_{x_i} \\forall (t_{x_i} \\in ds)$ -->\n",
    "3. Get average of the vector representation of the labelled dataset.\n",
    "4. Calculate cosine similarity for all text, with the average vector representation calculated in 3.\n",
    "6. Calculate similarity aggregations for $ds_{AT>0}, ds_{AT=0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAPTOP_TRAIN = '/home/gauneg/llm_experiments/ds_for_generation/ft_LLM_gen/laptop_train.csv'\n",
    "RESTAURANT_TRAIN = '/home/gauneg/llm_experiments/ds_for_generation/ft_LLM_gen/restaurant_train.csv'\n",
    "\n",
    "laptop_df = pd.read_csv(LAPTOP_TRAIN)\n",
    "restaurant_df = pd.read_csv(RESTAURANT_TRAIN)\n",
    "\n",
    "# # datadict = {'train': df_train.values, 'valid': df_valid.values, 'combined': np.vstack((df_train.values, df_valid.values))}\n",
    "# datadict = {'laptop_gold': laptop_df.values, 'restaurant_gold':restaurant_df.values}\n",
    "# tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')\n",
    "\n",
    "def split_sim_calc(df, sim_arr):\n",
    "    lab_text = []\n",
    "    un_lab_text = []\n",
    "    p_enc = PhraseEncoder()\n",
    "    p_enc.pre_calc_average(sim_arr)\n",
    "    for text, terms in df.values:\n",
    "        terms_parsed = ast.literal_eval(terms)\n",
    "        if len(terms_parsed)>0:\n",
    "            lab_text.append([text, terms, p_enc.calculate_cosine_sim_avg(text)])\n",
    "        else:\n",
    "            un_lab_text.append([text, terms, p_enc.calculate_cosine_sim_avg(text)])\n",
    "    return lab_text, un_lab_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating sim array, text used as ref for similarity\n",
    "\n",
    "term_list = []\n",
    "text_list = []\n",
    "for text, terms in restaurant_df.values:\n",
    "     terms_parsed = ast.literal_eval(terms)\n",
    "     for term in terms_parsed:\n",
    "          term_list.append(term[0].lower())\n",
    "     if len(terms_parsed)>0:\n",
    "          text_list.append(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:00, 70.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unlabelled 0.22226852 0.21172869\n",
      "labelled 0.30443174 0.32107848\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# lab_text, un_lab_text = split_sim_calc(restaurant_df, list(set(term_list)))\n",
    "# un_lab_df = pd.DataFrame(un_lab_text, columns=['text', 'terms', 'sim_score'])\n",
    "# lab_df = pd.DataFrame(lab_text, columns=['text', 'terms', 'sim_score'])\n",
    "# ux = un_lab_df['sim_score']\n",
    "# lx = lab_df['sim_score']\n",
    "# print('unlabelled',   ux.mean(), ux.median())\n",
    "# print('labelled',   lx.mean(), lx.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [00:00, 39.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unlabelled 0.39309788 0.39394715\n",
      "labelled 0.46724558 0.47146356\n"
     ]
    }
   ],
   "source": [
    "lap_dir = '/home/gauneg/llm_experiments/ds_for_generation/selective_gen_laptop'\n",
    "res_dir = '/home/gauneg/llm_experiments/ds_for_generation/selective_gen_restaurant'\n",
    "lab_text, un_lab_text = split_sim_calc(restaurant_df, text_list)\n",
    "un_lab_df = pd.DataFrame(un_lab_text, columns=['text', 'terms', 'sim_score'])\n",
    "lab_df = pd.DataFrame(lab_text, columns=['text', 'terms', 'sim_score'])\n",
    "ux = un_lab_df['sim_score']\n",
    "lx = lab_df['sim_score']\n",
    "print('unlabelled',   ux.mean(), ux.median())\n",
    "print('labelled',   lx.mean(), lx.median())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To check if lab_sim better than text_sim\n",
    "The objective of similarity score calculation is to **assign higher score to the unlabelled text, which is similar to text with labels.** \n",
    "\n",
    "Calculate similarity for:\n",
    "1. All sim with average of unique aspect terms in labs.\n",
    "2. All sim with average of all aspect terms in labs\n",
    "3. All sim with average of the text of datapoints having labelled aspect terms.\n",
    "\n",
    "\n",
    "| Sno. | Method                   | Mean sim of unlabelled(L) | Mean sim of labelled(L) | Mean sim of unlabelled(R) | Mean sim of labelled (R) |\n",
    "| ---- | ------------------------ | ----------------------- | ------------------------- | ----------------------- | -------------------------- |\n",
    "| 1    | Unique Asp Terms         |    0.247                |    0.235                  |    0.222                |        0.304               |\n",
    "| 2    | All Asp Terms            |    0.233                |    0.230                  |    0.202                |        0.277               |\n",
    "| 3    | Text where asp in lab    |    0.389                |    0.434                  |    0.339                |        0.475               |\n",
    "\n",
    "The evidence points and justifies, the use of text for similarity measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "un_lab_df.to_csv(os.path.join(lap_dir, 'res_llm_unlab.csv'), index=False)\n",
    "lab_df.to_csv(os.path.join(lap_dir, 'res_llm_lab.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selective_generation.semantic_enc import PhraseEncoder\n",
    "penc = PhraseEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 384])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rep = penc.encode_sentences(['this that them'])\n",
    "rep[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gauneg/anaconda3/envs/torch_test/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "msg = \"Adding description\"\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "\n",
    "from asp_pol_dataset import AspectDataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from self_eval import calc_labs\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from prompts import prompt_dict\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "# from evaluations_extracted_terms import evaluate_with_dataset, log_res\n",
    "from datetime import datetime\n",
    "\n",
    "def convert_ids_text(id_arr, tokenizer):\n",
    "    # print(id_arr)\n",
    "    toks = tokenizer.convert_ids_to_tokens(id_arr, skip_special_tokens=True)\n",
    "    return tokenizer.convert_tokens_to_string(toks).strip()\n",
    "\n",
    "LAPTOP_GOLD_PATH = '/home/gauneg/hester_letter/all_combined_ds/aspect_sentiment_polarity/laptop_2014/test.csv'\n",
    "RESTAURANT_GOLD_PATH = '/home/gauneg/hester_letter/all_combined_ds/aspect_sentiment_polarity/restaurant_2014/test.csv'\n",
    "BATCH_SIZE = 32\n",
    "df_lap = pd.read_csv(LAPTOP_GOLD_PATH)\n",
    "df_res = pd.read_csv(RESTAURANT_GOLD_PATH)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(f'google/flan-t5-large')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(f'google/flan-t5-large')\n",
    "datadict = {\n",
    "    \"laptop_gold\": DataLoader(AspectDataset(df_lap.values, tokenizer), batch_size=BATCH_SIZE), \n",
    "    \"restaurant_gold\":  DataLoader(AspectDataset(df_res.values, tokenizer), batch_size=BATCH_SIZE)\n",
    "    }\n",
    "\n",
    "\n",
    "def infer_res(dataloader, model, tokenizer):\n",
    "    fin_lab_pred = []\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        gold_labels = batch['labels']\n",
    "        pred_labels = model.generate(input_ids.to(device))\n",
    "        for i in range(input_ids.shape[0]):\n",
    "            input_txt = convert_ids_text(input_ids[i], tokenizer)\n",
    "            pred_txt = convert_ids_text(pred_labels[i], tokenizer)\n",
    "            gold_txt = convert_ids_text(gold_labels[i], tokenizer)\n",
    "            fin_lab_pred.append([input_txt, pred_txt, gold_txt])    \n",
    "    return fin_lab_pred \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21 [00:00<?, ?it/s]/home/gauneg/anaconda3/envs/torch_test/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 21/21 [02:05<00:00,  5.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['laptop_gold', 0.6891977807435996, 0.6972477064220184, 0.6015487827327012, 0.5199644581049252, 0.48545854056768295, 0.41650798177103965]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/36 [00:00<?, ?it/s]/home/gauneg/anaconda3/envs/torch_test/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 36/36 [03:35<00:00,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['restaurant_gold', 0.8142065371292829, 0.7839506172839507, 0.7170422616825931, 0.6206317976832612, 0.47586342229199374, 0.4337858946080941]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for k, dloader in datadict.items():\n",
    "    metric_arr = []\n",
    "\n",
    "    \n",
    "    res_arr = infer_res(dloader, model, tokenizer)\n",
    "    df_res = pd.DataFrame(res_arr, columns=['text', 'pred_y', 'true_y'])\n",
    "    metric_arr.append([k, \n",
    "                    precision_score(df_res['true_y'], df_res['pred_y'], zero_division=0, average='weighted'), \n",
    "                    recall_score(df_res['true_y'], df_res['pred_y'], zero_division=0, average='weighted'), \n",
    "                    f1_score(df_res['true_y'], df_res['pred_y'], zero_division=0, average='weighted'),\n",
    "                    precision_score(df_res['true_y'], df_res['pred_y'], zero_division=0, average='macro'),\n",
    "                    recall_score(df_res['true_y'], df_res['pred_y'], zero_division=0, average='macro'), \n",
    "                    f1_score(df_res['true_y'], df_res['pred_y'], zero_division=0, average='macro'),\n",
    "                    ])\n",
    "    print(metric_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[['laptop_gold_small', 0.5350237852531431, 0.6269113149847095, 0.5444088228956129, 0.341468253968254, 0.4513910842956671, 0.35502797404115227]]\n",
    "[['restaurant_gold_small', 0.6973597343675738, 0.7486772486772487, 0.68581841228952, 0.4274241229798975, 0.4649234693877551, 0.3929289480213001]]\n",
    "\n",
    "[['laptop_gold_base', 0.6661623101947468, 0.6880733944954128, 0.6625595767091559, 0.4669860926826871, 0.5092119757956064, 0.47138912288999]]\n",
    "[['restaurant_gold_base', 0.7372249635046139, 0.753968253968254, 0.7379939904631262, 0.4649688527096232, 0.5024038461538461, 0.47276313303099016]]\n",
    "\n",
    "[['laptop_gold_large', 0.6891977807435996, 0.6972477064220184, 0.6015487827327012, 0.5199644581049252, 0.48545854056768295, 0.41650798177103965]]\n",
    "[['restaurant_gold_large', 0.8142065371292829, 0.7839506172839507, 0.7170422616825931, 0.6206317976832612, 0.47586342229199374, 0.4337858946080941]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b706e822070771a14a9c94eb298c9282973d0466a7230323989ca595c84de93a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
